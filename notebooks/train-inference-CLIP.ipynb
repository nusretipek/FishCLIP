{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82a56404-c238-4bb2-ad46-f361a84a0976",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CLIP training script.\n",
    "\"\"\"\n",
    "\n",
    "# import libraries\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from tabulate import tabulate\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "import clip\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "# import taxa from parent directory\n",
    "sys.path.insert(0, str(Path.cwd().resolve().parent))\n",
    "from fishTaxa import taxaTuples\n",
    "\n",
    "# paths (anchor to repo root one level up)\n",
    "NOTEBOOK_DIR: Path = Path.cwd().resolve()\n",
    "ROOT_DIR: Path = NOTEBOOK_DIR.parent\n",
    "\n",
    "# constants\n",
    "BATCH_SIZE: int = 16\n",
    "EPOCHS: int = 15\n",
    "LEARNING_RATE: float = 1e-5\n",
    "WEIGHT_DECAY: float = 1e-3\n",
    "BETA1: float = 0.9\n",
    "BETA2: float = 0.98\n",
    "EVAL_FOLDER: str = str(ROOT_DIR / \"zeroCLIP\")\n",
    "DATASET_ROOT: str = str(ROOT_DIR / \"dataCLIP\")\n",
    "MODEL_NAME: str = \"RN101\"\n",
    "SEED: int = 0\n",
    "ENABLE_REPRODUCIBILITY = True\n",
    "\n",
    "# reproducibility settings\n",
    "if ENABLE_REPRODUCIBILITY:\n",
    "    random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d4e8087-8b76-40e6-89b5-267fb75c5b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select device\n",
    "device: str = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# load pre-trained CLIP model\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(MODEL_NAME, device=device, jit=False)\n",
    "\n",
    "\n",
    "# helper functions\n",
    "def makeCaptions(binom: str, common: str, cls: str, order: str, family: str, genus: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Create simple caption candidates for a species.\n",
    "\n",
    "    Args:\n",
    "        binom: Scientific binomial name.\n",
    "        common: Common name.\n",
    "        cls: Taxonomic class.\n",
    "        order: Taxonomic order.\n",
    "        family: Taxonomic family.\n",
    "        genus: Genus name.\n",
    "\n",
    "    Returns:\n",
    "        A list of caption strings.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        f\"{binom}\",\n",
    "        f\"a fish from family {family}\",\n",
    "        f\"a fish from order {order}\",\n",
    "        f\"a fish from class {cls}\",\n",
    "    ]\n",
    "\n",
    "\n",
    "def buildClipLists(root: str = DATASET_ROOT) -> Tuple[List[str], List[List[str]]]:\n",
    "    \"\"\"\n",
    "    Build parallel lists of image paths and caption options.\n",
    "\n",
    "    Args:\n",
    "        root: Dataset root containing subfolders per species.\n",
    "\n",
    "    Returns:\n",
    "        Tuple (imagePaths, textOptions) aligned by index.\n",
    "    \"\"\"\n",
    "    root = Path(root)\n",
    "    imagePaths, textOptions = [], []\n",
    "    for species in taxaTuples:\n",
    "        binom, common, cls, order, family, genus = species\n",
    "        captions = makeCaptions(binom, common, cls, order, family, genus)\n",
    "        folderPath = root / binom.replace(\" \", \"_\")\n",
    "        for img in sorted(folderPath.glob(\"*\")):\n",
    "            if img.suffix.lower() in {\".jpg\", \".jpeg\", \".png\", \".webp\"}:\n",
    "                imagePaths.append(str(img))\n",
    "                textOptions.append(captions)\n",
    "    assert len(imagePaths) == len(textOptions)\n",
    "    return imagePaths, textOptions\n",
    "\n",
    "\n",
    "class CLIPDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Minimal dataset yielding (imageTensor, textTokens) pairs for CLIP.\n",
    "\n",
    "    Attributes:\n",
    "        imagePath: List of image file paths.\n",
    "        textOptions: List of caption lists per image.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, listImagePath: List[str], listTxtOptions: List[List[str]]):\n",
    "        self.imagePath: List[str] = listImagePath\n",
    "        self.textOptions: List[List[str]] = listTxtOptions\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.imagePath)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        imgPath = self.imagePath[idx]\n",
    "        captionList = self.textOptions[idx]\n",
    "        caption = random.choice(captionList)\n",
    "\n",
    "        img = Image.open(imgPath)\n",
    "        img = ImageOps.exif_transpose(img)\n",
    "        if img.mode == \"P\" and (\"transparency\" in img.info or img.info.get(\"transparency\") is not None):\n",
    "            img = img.convert(\"RGBA\").convert(\"RGB\")\n",
    "        else:\n",
    "            img = img.convert(\"RGB\")\n",
    "        img.load()\n",
    "\n",
    "        image = preprocess(img)\n",
    "        title = clip.tokenize(caption)[0]\n",
    "        return image, title\n",
    "\n",
    "\n",
    "# build dataset and dataloader\n",
    "imageData, textData = buildClipLists(DATASET_ROOT)\n",
    "dataset = CLIPDataset(imageData, textData)\n",
    "trainDataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671187af-1e7b-41d3-9860-196d14b790f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero-shot evaluation prompts\n",
    "prompts = [\n",
    "    \"Salmonidae\",\n",
    "    \"Sphyraenidae\",\n",
    "    \"Pomacanthidae\",\n",
    "    \"Epinephelidae\",\n",
    "    \"Moronidae\",\n",
    "    \"Gymnotidae\",]\n",
    "    \n",
    "textTokens = clip.tokenize(prompts).to(device)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def clipPredict(imagePath: str, textTokens: torch.Tensor, texts: List[str], model: torch.nn.Module,\n",
    "                preprocess, device: str, topk: int = 5,) -> List[Tuple[float, str]]:\n",
    "    \"\"\"\n",
    "    Run CLIP zero-shot prediction for a single image.\n",
    "\n",
    "    Args:\n",
    "        imagePath: Path to image file.\n",
    "        textTokens: Tokenized prompt tensor.\n",
    "        texts: Original prompt strings (for labels).\n",
    "        model: CLIP model.\n",
    "        preprocess: Image transform function.\n",
    "        device: Device string (e.g., 'cuda:0' or 'cpu').\n",
    "        topk: Number of top classes to return.\n",
    "\n",
    "    Returns:\n",
    "        List of (score, label) pairs sorted by score desc.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    img = Image.open(imagePath)\n",
    "    image = preprocess(img).unsqueeze(0).to(device)\n",
    "    logitsPerImage, _ = model(image, textTokens)\n",
    "    probs = logitsPerImage.softmax(dim=-1).squeeze(0)\n",
    "    k = min(topk, len(texts))\n",
    "    scores, idx = torch.topk(probs, k=k, largest=True, sorted=True)\n",
    "    return [(float(scores[i]), texts[int(idx[i])]) for i in range(k)]\n",
    "\n",
    "\n",
    "def runEvaluation(imgFolder: str) -> None:\n",
    "    \"\"\"\n",
    "    Evaluate a folder of images using zero-shot CLIP prompts.\n",
    "\n",
    "    Args:\n",
    "        imgFolder: Folder containing images to evaluate.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(imgFolder):\n",
    "        print(f\"{imgFolder} does not exist.\")\n",
    "        return\n",
    "    if not len(glob.glob(f\"{imgFolder}/*\")):\n",
    "        print(f\"{imgFolder} is empty.\")\n",
    "        return\n",
    "\n",
    "    rows = []\n",
    "    for name in glob.glob(f\"{imgFolder}/*\"):\n",
    "        results = clipPredict(name, textTokens, prompts, model, preprocess, device, topk=3)\n",
    "        nameSplit = os.path.basename(name).split(\".\")[0].split(\"_\")\n",
    "        row = [f\"{nameSplit[0]} {nameSplit[1]}\"] + [nameSplit[2]] + [f\"{label} ({score:.3f})\" for score, label in results]\n",
    "        rows.append(row)\n",
    "\n",
    "    headers = [\"Image\", \"True Family\", \"Top-1\", \"Top-2\", \"Top-3\"]\n",
    "    print(tabulate(rows, headers=headers, tablefmt=\"fancy_grid\"), \"\\n\")\n",
    "\n",
    "\n",
    "# initial evaluation (before fine-tuning)\n",
    "print(\"=\" * 50)\n",
    "print(\"INITIAL EVALUATION (Before fine-tuning)\")\n",
    "print(\"=\" * 50)\n",
    "runEvaluation(EVAL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c486ce26-dead-46e4-a606-4bef5076fc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune CLIP model\n",
    "model = model.float().to(device)\n",
    "lossImg = nn.CrossEntropyLoss()\n",
    "lossTxt = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=(BETA1, BETA2), eps=1e-6, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# training loop\n",
    "print(\"=\" * 50)\n",
    "print(\"FINE-TUNING CLIP MODEL\")\n",
    "print(\"=\" * 50)\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epochLossSum = 0.0\n",
    "    epochCount = 0\n",
    "    stepIdx = 0\n",
    "    print(15 * \"-\", f\"Epoch {epoch+1}\", 15 * \"-\")\n",
    "    for batch in trainDataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Unpack the batch: (images, texts)\n",
    "        # images: float tensor (B, 3, H, W); texts: token ids (B, ctx_len)\n",
    "        images, texts = batch\n",
    "\n",
    "        # Move tensors to the selected device\n",
    "        images = images.to(device)\n",
    "        texts = texts.to(device)\n",
    "\n",
    "        # Forward through CLIP\n",
    "        logitsPerImage, logitsPerText = model(images, texts)\n",
    "        logitsPerImage = logitsPerImage.contiguous()\n",
    "        logitsPerText = logitsPerText.contiguous()\n",
    "\n",
    "        # Ground-truth matches along the batch diagonal\n",
    "        groundTruth = torch.arange(len(images), dtype=torch.long, device=device)\n",
    "        totalLoss = (lossImg(logitsPerImage, groundTruth) + lossTxt(logitsPerText, groundTruth)) / 2\n",
    "\n",
    "        # Backpropagation and optimizer step\n",
    "        totalLoss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track running loss statistics\n",
    "        bs = images.size(0)\n",
    "        epochLossSum += float(totalLoss.detach()) * bs\n",
    "        epochCount += bs\n",
    "        \n",
    "        # Verbose\n",
    "        stepIdx += 1\n",
    "        if stepIdx % 10 == 0:\n",
    "            runningAvg = epochLossSum / max(1, epochCount)\n",
    "            print(f\"Epoch {epoch+1} | Step {stepIdx}/{len(trainDataloader)} | Running average step loss: {runningAvg:.4f}\")\n",
    "    epochAvg = epochLossSum / max(1, epochCount)\n",
    "    print(f\"Epoch {epoch+1} | Epoch average loss: {epochAvg:.4f}\\n\")\n",
    "    runEvaluation(EVAL_FOLDER)\n",
    "\n",
    "\n",
    "# save checkpoint\n",
    "torch.save(\n",
    "    {\n",
    "        \"epoch\": EPOCHS,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"loss\": totalLoss,\n",
    "    },\n",
    "    f\"{MODEL_NAME}_{EPOCHS}.pt\",\n",
    ")\n",
    "print(f\"Training complete and model saved to: ./{MODEL_NAME}_{EPOCHS}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f31bdcc-7ed1-4346-bb86-90ac05f79fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME: str = \"RN101\"\n",
    "MODEL_PATH: str = \"RN101_15.pt\"\n",
    "IMAGE_PATH: str = str(ROOT_DIR / \"zeroCLIP\" / \"Sphyraena_novaehollandiae_Sphyraenidae.jpg\")\n",
    "PROMPTS: list[str] = [\"Salmonidae\", \"Sphyraenidae\", \"Pomacanthidae\",\n",
    "                      \"Epinephelidae\", \"Moronidae\", \"Gymnotidae\",]\n",
    "TOP_K: int = 3\n",
    "\n",
    "\n",
    "def loadModel(modelPath: Optional[str], device: str) -> Tuple[torch.nn.Module, any]:\n",
    "    \"\"\"\n",
    "    Load CLIP model (optionally loading fine-tuned weights).\n",
    "\n",
    "    Args:\n",
    "        modelPath: Optional path to a checkpoint file (.pt or .pth).\n",
    "        device: Torch device string ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        (model, preprocess)\n",
    "    \"\"\"\n",
    "    model, preprocess = clip.load(MODEL_NAME, device=device, jit=False)\n",
    "\n",
    "    if modelPath:\n",
    "        print(f\"Loading checkpoint from: {modelPath}\")\n",
    "        ckpt = torch.load(modelPath, weights_only=False, map_location=device)\n",
    "\n",
    "        # Look for correct key\n",
    "        if \"model_state_dict\" in ckpt:\n",
    "            state_dict = ckpt[\"model_state_dict\"]\n",
    "        elif \"state_dict\" in ckpt:\n",
    "            state_dict = ckpt[\"state_dict\"]\n",
    "        else:\n",
    "            state_dict = ckpt\n",
    "\n",
    "        try:\n",
    "            model.load_state_dict(state_dict, strict=False)\n",
    "            print(\"Model weights loaded successfully.\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: could not load some weights â†’ {e}\")\n",
    "\n",
    "    return model, preprocess\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def clipPredict(\n",
    "    imagePath: str,\n",
    "    textTokens: torch.Tensor,\n",
    "    texts: List[str],\n",
    "    model: torch.nn.Module,\n",
    "    preprocess,\n",
    "    device: str,\n",
    "    topk: int = 5,\n",
    ") -> List[Tuple[float, str]]:\n",
    "    \"\"\"\n",
    "    Run CLIP zero-shot prediction for a single image.\n",
    "\n",
    "    Args:\n",
    "        imagePath: Path to image file.\n",
    "        textTokens: Tokenized prompts tensor.\n",
    "        texts: Original prompts for label mapping.\n",
    "        model: CLIP model instance.\n",
    "        preprocess: CLIP image preprocessing transform.\n",
    "        device: Torch device string.\n",
    "        topk: Number of top predictions.\n",
    "\n",
    "    Returns:\n",
    "        List of (score, label) sorted by score descending.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    img = Image.open(imagePath)\n",
    "    image = preprocess(img).unsqueeze(0).to(device)\n",
    "    logitsPerImage, _ = model(image, textTokens)\n",
    "    probs = logitsPerImage.softmax(dim=-1).squeeze(0)\n",
    "    k = min(topk, len(texts))\n",
    "    scores, idx = torch.topk(probs, k=k, largest=True, sorted=True)\n",
    "    return [(float(scores[i]), texts[int(idx[i])]) for i in range(k)]\n",
    "\n",
    "# inference\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = loadModel(MODEL_PATH, device)\n",
    "\n",
    "texts: List[str] = PROMPTS\n",
    "tokens = clip.tokenize(texts).to(device)\n",
    "results = clipPredict(IMAGE_PATH, tokens, texts, model, preprocess, device, topk=TOP_K)\n",
    "\n",
    "print(f\"Image: {Path(IMAGE_PATH).name}\")\n",
    "for rank, (score, label) in enumerate(results, start=1):\n",
    "    print(f\"Top-{rank}: {label} ({score:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
